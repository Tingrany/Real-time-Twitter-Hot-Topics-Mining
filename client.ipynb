{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from pyspark import SparkContext\n",
    "# from pyspark.streaming import StreamingContext\n",
    "# import nltk\n",
    "# import heapq\n",
    "# import numpy as np\n",
    "# import datetime\n",
    "# import json\n",
    "# from pyspark.mllib.linalg import Vectors\n",
    "# import math\n",
    "# import string\n",
    "# from nltk.corpus import stopwords, wordnet\n",
    "# from collections import Counter\n",
    "# from pyspark.mllib.clustering import LDA, LDAModel\n",
    "# import heapq\n",
    "# import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # interval is the window interval in minutes\n",
    "# def write_file(topic_word_list, sample_tweet, interval):\n",
    "#     localtime = time.asctime(time.localtime(time.time()))\n",
    "#     file_name = get_file_name(localtime, interval)\n",
    "#     topic_number = len(topic_word_list)\n",
    "#     data_to_write = {'topic_number': topic_number, 'topics': {}, 'sample_tweet': {}}\n",
    "#     for i in range(topic_number):\n",
    "#         data_to_write['topics']['topic_'+str(i)] = topic_word_list[i]\n",
    "#         data_to_write['sample_tweet']['topic_'+str(i)] = sample_tweet[i]\n",
    "# #     print(data_to_write)\n",
    "#     write_file = open(file_name, 'w')\n",
    "#     write_file.write(str(data_to_write))\n",
    "#     write_file.close()\n",
    "\n",
    "# def get_file_name(localtime, interval):\n",
    "#     time_stamp = localtime.split(' ')\n",
    "#     tmp = time_stamp[4].split(':')\n",
    "#     hour_minute = tmp[0]+':'+str(int(int(tmp[1])/interval)*interval)\n",
    "#     return '_'.join([time_stamp[1], time_stamp[3], hour_minute, time_stamp[5]])+'.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 0))\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4b05d28726bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tingran/anaconda2/lib/python2.7/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# Import top-level functionality into top-level namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemoize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatstruct\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tingran/anaconda2/lib/python2.7/site-packages/nltk/collocations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContingencyMeasures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBigramAssocMeasures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrigramAssocMeasures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspearman\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mranks_from_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspearman_correlation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tingran/anaconda2/lib/python2.7/site-packages/nltk/metrics/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \"\"\"\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m from nltk.metrics.scores import          (accuracy, precision, recall, f_measure,\n\u001b[0m\u001b[1;32m     17\u001b[0m                                           log_likelihood, approxrand)\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusionmatrix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfusionMatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tingran/anaconda2/lib/python2.7/site-packages/nltk/metrics/scores.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbetai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mbetai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tingran/anaconda2/lib/python2.7/site-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0minterface\u001b[0m \u001b[0mpackage\u001b[0m \u001b[0mrpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m \"\"\"\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import nltk\n",
    "import heapq\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import math\n",
    "import string\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from collections import Counter\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "import heapq\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess():\n",
    "    words = stopwords.words('english')\n",
    "    for w in ['!',',','.','?','-s','-ly','</s>','s','@','&','$','#']:\n",
    "        words.append(w)\n",
    "    return words\n",
    "#     filtered_words = [word for word in (line for line in text) if word not in words]\n",
    "#     return filter_words\n",
    "\n",
    "def get_tokens(text):\n",
    "    lowers = text.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "    no_punctuation = lowers.translate(remove_punctuation_map)\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "# def write_log(x):\n",
    "#     logfile = \"log\"\n",
    "#     with open(logfile,\"a\") as f:\n",
    "#         f.write(x)\n",
    "#         f.write('\\n')\n",
    "#     return x\n",
    "\n",
    "def main(x):\n",
    "    \n",
    "    rst = []\n",
    "    for i in range(len(x[1])):\n",
    "        if i%4 != 3:\n",
    "            continue\n",
    "        else:\n",
    "            rst.append([i/4+1, [x[1][i]]])\n",
    "    return rst\n",
    "\n",
    "def main2(rst, interval):\n",
    "    \n",
    "    import nltk\n",
    "    import heapq\n",
    "    import numpy as np\n",
    "    import datetime\n",
    "    import json\n",
    "    from pyspark.mllib.linalg import Vectors\n",
    "    import math\n",
    "    import string\n",
    "    from nltk.corpus import stopwords, wordnet\n",
    "    from collections import Counter\n",
    "    from pyspark.mllib.clustering import LDA, LDAModel\n",
    "    import heapq\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    for w in ['!',',','.','?','-s','-ly','</s>','s','@','&','$','#']:\n",
    "        stop_words.append(w)\n",
    "    \n",
    "    sno = nltk.stem.SnowballStemmer('english')\n",
    "    all_words = []\n",
    "    \n",
    "    if rst == None:\n",
    "        return [1,2,3]\n",
    "    \n",
    "    for entry in rst:\n",
    "        all_words += get_tokens(entry[1][0])\n",
    "    \n",
    "    all_words = [word for word in all_words if word not in stop_words]\n",
    "    all_words2 = [x for x in all_words if x[0:8] != u'httpstco'] \n",
    "    raw_wordlist = []\n",
    "    \n",
    "    for word in all_words2:\n",
    "        if word.isalpha() == False or len(word) <= 3:\n",
    "            continue\n",
    "        word = sno.stem(word)\n",
    "        if wordnet.synsets(word):  \n",
    "            raw_wordlist.append(word)\n",
    "    count = Counter(raw_wordlist)\n",
    "    word_list = count.most_common(int(math.floor(0.5*len(count))))\n",
    "    \n",
    "    dictionary2 = {}\n",
    "    for i in range(len(word_list)):\n",
    "        dictionary2[i] = word_list[i][0]\n",
    "    print \"The length of my dictionary2 is  \", len(dictionary2)\n",
    "    dictionary = {}\n",
    "    for i in range(len(word_list)):\n",
    "        dictionary[word_list[i][0]] = (i,word_list[i][1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    line = []\n",
    "    bigMatrix = []\n",
    "    \n",
    "    for entry in rst:\n",
    "        line = get_tokens(entry[1][0])\n",
    "        temp = np.zeros(shape=(1,len(word_list)))\n",
    "        ######################### rebuild the rst\n",
    "#         line = [word for word in line if word not in stop_words]\n",
    "#         line = [word for word in line if word[0:8] != u'httpstco']\n",
    "        #########################\n",
    "        for word in line:\n",
    "            word = sno.stem(word)\n",
    "            if(wordnet.synsets(word)):\n",
    "                if word in dictionary:\n",
    "                    temp[0][dictionary[word][0]] += 1\n",
    "        bigMatrix.append(temp[0])\n",
    "    \n",
    "    Matrix = np.array(bigMatrix) \n",
    "    ############################################\n",
    "    \n",
    "    X = Matrix.T\n",
    "    K = 5\n",
    "    T = 100\n",
    "    def LDA(X, K, T, n, dictionary):\n",
    "        W = np.random.random((X.shape[0], K)) + 1\n",
    "        H = np.random.random((K, X.shape[1])) + 1\n",
    "        for i in range(T):\n",
    "\n",
    "            print(i)\n",
    "            temp = X/(W.dot(H) + 10 ** -16)\n",
    "\n",
    "            #s = W.sum(axis = 0)\n",
    "            W_t_n = W.T/W.sum(axis = 0).reshape(1,-1).T\n",
    "            H = H * W_t_n.dot(temp)\n",
    "\n",
    "            temp = X/(W.dot(H) + 10 ** -16)\n",
    "\n",
    "        #s = H.sum(axis = 1)\n",
    "            H_t_n = H.T/H.sum(axis = 1).reshape(1,-1)\n",
    "            W = W * temp.dot(H_t_n)\n",
    "        #temp = W.dot(H) + 10 ** - 16\n",
    "        #diff = -X * np.log(temp) + temp\n",
    "        #f.append(diff.sum())\n",
    "        W_n = W.T/W.sum(axis = 0).reshape(1,-1).T\n",
    "        topics = []\n",
    "        for i in range(K):\n",
    "            topics.append(heapq.nlargest(n, range(len(W_n[i])), W_n[i].take))\n",
    "    \n",
    "        topic_words_list = []\n",
    "\n",
    "        for i in range(K):\n",
    "            topic_words = []\n",
    "            for index in topics[i]:\n",
    "                #topic_words.append(np.array([dictionary[index], round(W_n[i][index], 4)]))\n",
    "                topic_words.append([dictionary[index].encode('utf-8'), round(W_n[i][index], 4)])\n",
    "            topic_words_list.append(topic_words)\n",
    "        \n",
    "        samples = []\n",
    "        for i in range(K):\n",
    "            samples.append(heapq.nlargest(1, range(len(H[i])), H[i].take))\n",
    "        \n",
    "        sample_tweet = []\n",
    "        \n",
    "        ###\n",
    "        for index in samples:\n",
    "            sample_tweet.append(rst[index[0]][1][0].encode('utf-8'))\n",
    "            #sample_tweet.append(rst[0][1][0])\n",
    "        \n",
    "        return W, H, topics, W_n.shape, topic_words_list, sample_tweet, H.shape, X.shape\n",
    "        \n",
    "    def get_file_name(localtime, interval):\n",
    "        time_stamp = localtime.split(' ')\n",
    "        tmp = time_stamp[4].split(':')\n",
    "        if int(time_stamp[3]) < 10:\n",
    "            time_stamp[3] = '0' + time_stamp[3]\n",
    "        if int(tmp[0]) < 10:\n",
    "            tmp[0] = '0' + tmp[0]\n",
    "        if int(int(tmp[1])/interval)*interval < 10:\n",
    "            hour_minute = tmp[0] + '|0' + str(int(int(tmp[1])/interval)*interval)\n",
    "        else:\n",
    "            hour_minute = tmp[0]+'|'+str(int(int(tmp[1])/interval)*interval)\n",
    "        return '_'.join([time_stamp[1], time_stamp[3], hour_minute, time_stamp[5]])+'.txt'\n",
    "    \n",
    "    def write_file(topic_word_list, sample_tweet, interval):\n",
    "        localtime = time.asctime(time.localtime(time.time()))\n",
    "        file_name = get_file_name(localtime, interval)\n",
    "        topic_number = len(topic_word_list)\n",
    "        data_to_write = {'topic_number': topic_number, 'topics': {}, 'sample_tweet': {}}\n",
    "        for i in range(topic_number):\n",
    "            data_to_write['topics']['topic_'+str(i)] = topic_word_list[i]\n",
    "            data_to_write['sample_tweet']['topic_'+str(i)] = sample_tweet[i]\n",
    "    #     print(data_to_write)\n",
    "        write_file = open(file_name, 'w')\n",
    "        write_file.write(str(data_to_write))\n",
    "        write_file.close()\n",
    "        \n",
    "        \n",
    "    if(len(X)==0):\n",
    "        return rst\n",
    "    \n",
    "    # interval is the window interval in minutes\n",
    "    lda_return = LDA(X, K, T, min(5,int(len(dictionary2))), dictionary2)\n",
    "    write_file(lda_return[4], lda_return[5], interval)\n",
    "    \n",
    "    ###############\n",
    "    \n",
    "#     H = lda_return[1]\n",
    "#     v = H.sum(axis = 1)\n",
    "#     topic_sort = np.argsort(-v)\n",
    "#     plt.bar(range(K), v)\n",
    "#     plt.xticks(range(K), lda_return[4][:,0,0])\n",
    "#     plt.savefig(\"Topic_bar\", dpi = 600)\n",
    "    \n",
    "    \n",
    "    #################\n",
    "    \n",
    "    \n",
    "    #return [dictionary2,topics]\n",
    "    \n",
    "    return [rst,lda_return[6],lda_return[7]]\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    batch_interval = 1 # base time unit (in seconds)\n",
    "    window_length = 10 * batch_interval\n",
    "    frequency = 2 * batch_interval\n",
    "    try:\n",
    "        stc.stop()  \n",
    "    except: \n",
    "        pass\n",
    "    try:\n",
    "        spc.stop()  \n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "    spc = SparkContext(appName=\"OnlineLDA\")\n",
    "    stc = StreamingContext(spc, batch_interval)\n",
    "    stc.checkpoint(\"checkpoint\")\n",
    "    \n",
    "#     stop_words =preprocess()\n",
    "#     spc.broadcast(stop_words)\n",
    "    \n",
    "    lines = stc.socketTextStream('', 9000)\n",
    "    windows = lines.window(window_length, frequency)\n",
    "    \n",
    "    #implement LDA algorithm\n",
    "    line = windows.map(lambda x: (1,(2,x[12:]))).reduceByKey(lambda x,y: x+y).map(lambda x: main(x))\\\n",
    "    .map(lambda x:main2(x, frequency))\n",
    "    line.pprint()\n",
    "    stc.start()\n",
    "    stc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL4OSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103d63150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "i = range(5)\n",
    "v = [2.3,3,4,3.2,2]\n",
    "\n",
    "name_list = ['job','love','sports','news','summer']\n",
    "plt.bar(i, v)\n",
    "plt.xticks(i,name_list)\n",
    "\n",
    "plt.show()\n",
    "# ax.set_xticks(ind)  \n",
    "# ax.set_xticklabels(name_list)  \n",
    "# # labels  \n",
    "# ax.set_xlabel('Country')\n",
    "# ax.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3], [4, 5, 6, 7])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [0,1,2,3]\n",
    "y = [4,5,6,7]\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def preprocess():\n",
    "#     words = stopwords.words('english')\n",
    "#     for w in ['!',',','.','?','-s','-ly','</s>','s','@','&','$','#']:\n",
    "#         words.append(w)\n",
    "    \n",
    "#     filtered_words = [word for word in (line for line in text) if word not in words]\n",
    "#     return filter_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_tokens(x):\n",
    "#     lowers = x.lower()\n",
    "#     #remove the punctuation using the character deletion step of translate\n",
    "#     no_punctuation = lowers.translate(None, string.punctuation)\n",
    "#     tokens = nltk.word_tokenize(no_punctuation)\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def write_log(x):\n",
    "#     logfile = \"log\"\n",
    "#     with open(logfile,\"a\") as f:\n",
    "#         f.write(x)\n",
    "#         f.write('\\n')\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def main(x):\n",
    "#     rst = []\n",
    "#     for i in range(len(x[1])):\n",
    "#         if i%4 != 3:\n",
    "#             continue\n",
    "#         else:\n",
    "#             rst.append([i/3, [x[1][i]]])\n",
    "    \n",
    "#     filtered_words = preprocess()\n",
    "#     sno = nltk.stem.SnowballStemmer('english')\n",
    "#     all_words = []\n",
    "#     for entry in rst:\n",
    "#         all_words += get_tokens(entry[1][0])\n",
    "#     all_words2 = [x for x in all_words if x[0:8] != u'httpstco']\n",
    "#     raw_wordlist = [] \n",
    "#     for word in all_words2:\n",
    "#         if word.isalpha() == False or len(word) <= 3:\n",
    "#             continue\n",
    "#         word = sno.stem(word)\n",
    "#         if wordnet.synsets(word):  \n",
    "#             raw_wordlist.append(word)\n",
    "#     count = Counter(raw_wordlist)\n",
    "#     word_list = count.most_common(round(0.5*len(count)))\n",
    "    \n",
    "#     dictionary2 = {}\n",
    "#     for i in range(len(word_list)):\n",
    "#         dictionary2[i] = word_list[i][0]\n",
    "#     dictionary = {}\n",
    "#     for i in range(math.ceil(0.5*len(count))):\n",
    "#         dictionary[word_list[i][0]] = (i,word_list[i][1])\n",
    "\n",
    "#     line = []\n",
    "#     bigMatrix = []\n",
    "    \n",
    "#     for item in filtered_words:\n",
    "#         temp = np.zeros(shape=(1,int(math.ceil(0.5*len(count)))))\n",
    "#         line = get_tokens(item)\n",
    "#         line = [word for word in line if word not in stopwords.words('english')]\n",
    "#         line = [word for word in line if word[0:8] != u'httpstco']\n",
    "#         for word in line:\n",
    "#             word = sno.stem(word)\n",
    "#             if(wordnet.synsets(word)):\n",
    "#                 if word in dictionary:\n",
    "#                     temp[0][dictionary[word][0]] += 1\n",
    "#         bigMatrix.append(temp[0])\n",
    "    \n",
    "#     return bigMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o21.start.\n: java.io.IOException: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/streaming/util.py\", line 105, in dumps\n    func.func, func.rdd_wrap_func, func.deserializers)))\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/serializers.py\", line 464, in dumps\n    return cloudpickle.dumps(obj, 2)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 704, in dumps\n    cp.dump(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 148, in dump\n    return Pickler.dump(self, obj)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 224, in dump\n    self.save(obj)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 639, in _batch_appends\n    save(x)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 642, in _batch_appends\n    save(tmp[0])\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 639, in _batch_appends\n    save(x)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 642, in _batch_appends\n    save(tmp[0])\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 642, in _batch_appends\n    save(tmp[0])\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n    save(f_globals)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 655, in save_dict\n    self._batch_setitems(obj.iteritems())\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 692, in _batch_setitems\n    save(v)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n    save(f_globals)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 655, in save_dict\n    self._batch_setitems(obj.iteritems())\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 687, in _batch_setitems\n    save(v)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n    save(f_globals)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 655, in save_dict\n    self._batch_setitems(obj.iteritems())\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 692, in _batch_setitems\n    save(v)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 331, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 565, in save_reduce\n    \"args[0] from __newobj__ args has the wrong class\")\nPicklingError: args[0] from __newobj__ args has the wrong class\n\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)\n\tat org.apache.spark.streaming.api.python.TransformFunction.writeObject(PythonDStream.scala:100)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1128)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:441)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply$mcV$sp(DStreamGraph.scala:187)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply(DStreamGraph.scala:182)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply(DStreamGraph.scala:182)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)\n\tat org.apache.spark.streaming.DStreamGraph.writeObject(DStreamGraph.scala:182)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1128)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply$mcV$sp(Checkpoint.scala:150)\n\tat org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply(Checkpoint.scala:150)\n\tat org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply(Checkpoint.scala:150)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)\n\tat org.apache.spark.streaming.Checkpoint$.serialize(Checkpoint.scala:151)\n\tat org.apache.spark.streaming.StreamingContext.validate(StreamingContext.scala:525)\n\tat org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:573)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:556)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/streaming/util.py\", line 105, in dumps\n    func.func, func.rdd_wrap_func, func.deserializers)))\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/serializers.py\", line 464, in dumps\n    return cloudpickle.dumps(obj, 2)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 704, in dumps\n    cp.dump(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 148, in dump\n    return Pickler.dump(self, obj)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 224, in dump\n    self.save(obj)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 639, in _batch_appends\n    save(x)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 642, in _batch_appends\n    save(tmp[0])\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 639, in _batch_appends\n    save(x)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 642, in _batch_appends\n    save(tmp[0])\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 642, in _batch_appends\n    save(tmp[0])\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n    save(f_globals)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 655, in save_dict\n    self._batch_setitems(obj.iteritems())\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 692, in _batch_setitems\n    save(v)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n    save(f_globals)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 655, in save_dict\n    self._batch_setitems(obj.iteritems())\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 687, in _batch_setitems\n    save(v)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n    save(f_globals)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 655, in save_dict\n    self._batch_setitems(obj.iteritems())\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 692, in _batch_setitems\n    save(v)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 331, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 565, in save_reduce\n    \"args[0] from __newobj__ args has the wrong class\")\nPicklingError: args[0] from __newobj__ args has the wrong class\n\n\tat org.apache.spark.streaming.api.python.PythonTransformFunctionSerializer$.serialize(PythonDStream.scala:144)\n\tat org.apache.spark.streaming.api.python.TransformFunction$$anonfun$writeObject$1.apply$mcV$sp(PythonDStream.scala:101)\n\tat org.apache.spark.streaming.api.python.TransformFunction$$anonfun$writeObject$1.apply(PythonDStream.scala:100)\n\tat org.apache.spark.streaming.api.python.TransformFunction$$anonfun$writeObject$1.apply(PythonDStream.scala:100)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)\n\t... 63 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8b3ef1fc915a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#rdd = windows.map(lambda x:x.split(\" \")).map(lambda x: A(x)).filter(lambda x: x is not None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mstc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mstc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/streaming/context.pyc\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mStart\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mexecution\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \"\"\"\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mStreamingContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activeContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tingran/anaconda2/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tingran/anaconda2/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o21.start.\n: java.io.IOException: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/streaming/util.py\", line 105, in dumps\n    func.func, func.rdd_wrap_func, func.deserializers)))\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/serializers.py\", line 464, in dumps\n    return cloudpickle.dumps(obj, 2)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 704, in dumps\n    cp.dump(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 148, in dump\n    return Pickler.dump(self, obj)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 224, in dump\n    self.save(obj)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 639, in _batch_appends\n    save(x)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 642, in _batch_appends\n    save(tmp[0])\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 639, in _batch_appends\n    save(x)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 642, in _batch_appends\n    save(tmp[0])\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 642, in _batch_appends\n    save(tmp[0])\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n    save(f_globals)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 655, in save_dict\n    self._batch_setitems(obj.iteritems())\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 692, in _batch_setitems\n    save(v)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n    save(f_globals)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 655, in save_dict\n    self._batch_setitems(obj.iteritems())\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 687, in _batch_setitems\n    save(v)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n    save(f_globals)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 655, in save_dict\n    self._batch_setitems(obj.iteritems())\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 692, in _batch_setitems\n    save(v)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 331, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 565, in save_reduce\n    \"args[0] from __newobj__ args has the wrong class\")\nPicklingError: args[0] from __newobj__ args has the wrong class\n\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)\n\tat org.apache.spark.streaming.api.python.TransformFunction.writeObject(PythonDStream.scala:100)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1128)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:441)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply$mcV$sp(DStreamGraph.scala:187)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply(DStreamGraph.scala:182)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply(DStreamGraph.scala:182)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)\n\tat org.apache.spark.streaming.DStreamGraph.writeObject(DStreamGraph.scala:182)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1128)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply$mcV$sp(Checkpoint.scala:150)\n\tat org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply(Checkpoint.scala:150)\n\tat org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply(Checkpoint.scala:150)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)\n\tat org.apache.spark.streaming.Checkpoint$.serialize(Checkpoint.scala:151)\n\tat org.apache.spark.streaming.StreamingContext.validate(StreamingContext.scala:525)\n\tat org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:573)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:556)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/streaming/util.py\", line 105, in dumps\n    func.func, func.rdd_wrap_func, func.deserializers)))\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/serializers.py\", line 464, in dumps\n    return cloudpickle.dumps(obj, 2)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 704, in dumps\n    cp.dump(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 148, in dump\n    return Pickler.dump(self, obj)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 224, in dump\n    self.save(obj)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 639, in _batch_appends\n    save(x)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 642, in _batch_appends\n    save(tmp[0])\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 639, in _batch_appends\n    save(x)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 642, in _batch_appends\n    save(tmp[0])\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 554, in save_tuple\n    save(element)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 606, in save_list\n    self._batch_appends(iter(obj))\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 642, in _batch_appends\n    save(tmp[0])\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n    save(f_globals)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 655, in save_dict\n    self._batch_setitems(obj.iteritems())\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 692, in _batch_setitems\n    save(v)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n    save(f_globals)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 655, in save_dict\n    self._batch_setitems(obj.iteritems())\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 687, in _batch_setitems\n    save(v)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n    save(f_globals)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 655, in save_dict\n    self._batch_setitems(obj.iteritems())\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 692, in _batch_setitems\n    save(v)\n  File \"/Users/tingran/anaconda2/lib/python2.7/pickle.py\", line 331, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/tingran/Documents/2017_2018AcademicYear/spark-2.2.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 565, in save_reduce\n    \"args[0] from __newobj__ args has the wrong class\")\nPicklingError: args[0] from __newobj__ args has the wrong class\n\n\tat org.apache.spark.streaming.api.python.PythonTransformFunctionSerializer$.serialize(PythonDStream.scala:144)\n\tat org.apache.spark.streaming.api.python.TransformFunction$$anonfun$writeObject$1.apply$mcV$sp(PythonDStream.scala:101)\n\tat org.apache.spark.streaming.api.python.TransformFunction$$anonfun$writeObject$1.apply(PythonDStream.scala:100)\n\tat org.apache.spark.streaming.api.python.TransformFunction$$anonfun$writeObject$1.apply(PythonDStream.scala:100)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)\n\t... 63 more\n"
     ]
    }
   ],
   "source": [
    "# # Sliding window for twitter\n",
    "# if __name__ == \"__main__\":\n",
    "# #     if len(sys.argv) != 3:\n",
    "# #         raise IOError(\"Invalid usage; the correct format is:\\nwindow_count.py <hostname> <port>\")\n",
    "\n",
    "#     batch_interval = 1 # base time unit (in seconds)\n",
    "#     window_length = 10 * batch_interval\n",
    "#     frequency = 2 * batch_interval\n",
    "#     try:\n",
    "#         stc.stop()  \n",
    "#     except: \n",
    "#         pass\n",
    "#     try:\n",
    "#         spc.stop()  \n",
    "#     except: \n",
    "#         pass\n",
    "#     spc = SparkContext(appName=\"OnlineLDA\")\n",
    "#     stc = StreamingContext(spc, batch_interval)\n",
    "#     stc.checkpoint(\"checkpoint\")\n",
    "    \n",
    "#     lines = stc.socketTextStream('', 9001)\n",
    "#     windows = lines.window(window_length, frequency)\n",
    "    \n",
    "#     #implement LDA algorithm\n",
    "    \n",
    "#     line = windows.map(lambda x: (1,(2,x[12:]))).reduceByKey(lambda x,y: x+y).map(lambda x: main(x))\n",
    "#     #temp = windows.flatMap(lambda x: x[12:].strip().split(\" \"))\\\n",
    "#     #.map(lambda x: (x, 1))\\\n",
    "#     #.reduceByKey(lambda x,y: x + y)\n",
    "    \n",
    "#     #     temp = windows.flatMap(lambda x: x[12:].strip().split(' ')).filter(lambda x: x) \\\n",
    "# #                   .map(lambda x: (x, 1)) \\\n",
    "# #                   .reduceByKey(lambda x,y: x+y)\n",
    "#             #.map(lambda x: A(x))\n",
    "#             #.map(lambda x: word_clean(x)).filter(lambda x: x) \\\n",
    "#     #rdd = windows.map(lambda x:x.split(\" \")).map(lambda x: A(x)).filter(lambda x: x is not None)\n",
    "#     line.pprint()\n",
    "#     stc.start()\n",
    "#     stc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     stopWORD = preprocess()\n",
    "#     sno = nltk.stem.SnowballStemmer('english')\n",
    "#     all_words = []\n",
    "#     for entry in rst:\n",
    "#         lowers = entry[1][0].encode('utf-8').lower()\n",
    "#         #remove the punctuation using the character deletion step of translate\n",
    "#         no_punctuation = lowers.translate(None, string.punctuation)\n",
    "#         tokens = nltk.word_tokenize(no_punctuation)\n",
    "#         all_words += tokens\n",
    "#     all_words2 = [x for x in all_words if x[0:8] != u'httpstco']\n",
    "#     raw_wordlist = [] \n",
    "#     for word in all_words2:\n",
    "#         if word.isalpha() == False or len(word) <= 3:\n",
    "#             continue\n",
    "#         word = sno.stem(word)\n",
    "#         if wordnet.synsets(word):  \n",
    "#             raw_wordlist.append(word)\n",
    "#     count = Counter(raw_wordlist)\n",
    "#     word_list = count.most_common(round(0.5*len(count)))\n",
    "    \n",
    "#     dictionary2 = {}\n",
    "#     for i in range(len(word_list)):\n",
    "#         dictionary2[i] = word_list[i][0]\n",
    "#     dictionary = {}\n",
    "#     for i in range(round(0.5*len(count))):\n",
    "#         dictionary[word_list[i][0]] = (i,word_list[i][1])\n",
    "\n",
    "#     line = []\n",
    "#     bigMatrix = []\n",
    "    \n",
    "#     for item in filtered_words:\n",
    "#         temp = np.zeros(shape=(1,round(0.5*len(count))))\n",
    "#         line = get_tokens(item)\n",
    "#         line = [word for word in line if word not in stopWORD]\n",
    "#         line = [word for word in line if word[0:8] != u'httpstco']\n",
    "#         for word in line:\n",
    "#             word = sno.stem(word)\n",
    "#             if(wordnet.synsets(word)):\n",
    "#                 if word in dictionary:\n",
    "#                     temp[0][dictionary[word][0]] += 1\n",
    "#         bigMatrix.append(temp[0])\n",
    "# Sliding window for twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def A(x):\n",
    "#     rst = []\n",
    "#     if x[0] == u'{' or x[0] == u'}{':\n",
    "#         return None\n",
    "#     for item in x:\n",
    "#         if item[0:6] == u'\"text:\"':\n",
    "#             rst.append(item[7:])\n",
    "#             continue\n",
    "#         if item != u'':\n",
    "#             rst.append(item)\n",
    "#     return rst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
